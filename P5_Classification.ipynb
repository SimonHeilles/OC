{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo8kGAj1jZ4a4efycYdTvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimonHeilles/OC/blob/main/P5_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bD05T3q9aNf",
        "outputId": "0dd4582a-92fc-4f2d-a28d-804f731ae14e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbG5o31v5GvZ",
        "outputId": "3b1406ea-4006-49f3-8cd4-cfad88d79bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "\n",
        "#Preprocessing\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#LDA\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "#Feature Extractions\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow_hub as hub\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    TFDistilBertModel,\n",
        "    DistilBertConfig,\n",
        ")\n",
        "\n",
        "#Predictions\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Metrics\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "# Import\n",
        "url = 'https://raw.githubusercontent.com/SimonHeilles/OC/main/QueryResults%20(2).csv'\n",
        "data = pd.read_csv(url)\n",
        "df = data.copy()\n",
        "\n",
        "# Pre-processing\n",
        "df['Body'] = df['Body'].apply(lambda x: BeautifulSoup(x).get_text())\n",
        "df['Tags'] = df['Tags'].str.replace(\"<\", ' ')\n",
        "df['Tags'] = df['Tags'].str.replace(\">\", ' ')\n",
        "df['Tags'] = df['Tags'].str.split().str.join(\" \")\n",
        "\n",
        "text_columns = df[['Title', 'Body']]\n",
        "\n",
        "for column in text_columns:\n",
        "  df[column] = df[column].str.lower()\n",
        "\n",
        "for column in text_columns:\n",
        "  spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
        "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
        "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
        "              \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \"$\"]\n",
        "\n",
        "for char in spec_chars:\n",
        "    df[column] = df[column].str.replace(char, ' ')\n",
        "\n",
        "for column in text_columns:\n",
        "  df[column] = df[column].str.split().str.join(\" \")\n",
        "\n",
        "df2 = df.copy()\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "\n",
        "for column in text_columns:\n",
        "  df2[column] = df2[column].apply(lambda x: [str(word) for word in word_tokenize(x) if not word in cachedStopWords])\n",
        "\n",
        "for column in text_columns:\n",
        "  df2[column] = df2[column].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# NB : no stemming, doesn't really increase the results\n",
        "\n",
        "# Preparing the list of tags\n",
        "df_cv = df2.copy()\n",
        "df_cv['TitleBody'] = df_cv['Title'] + ' ' + df_cv['Body']\n",
        "tags_list = []\n",
        "\n",
        "for words_list in df_cv['Tags']:\n",
        "  tags_list.append(words_list.split())\n",
        "\n",
        "flat_list = [item for sublist in tags_list for item in sublist]\n",
        "\n",
        "Counter = Counter(flat_list)\n",
        "\n",
        "no_words = 20 # number of words we accept in the list of tags\n",
        "\n",
        "most_occur = Counter.most_common(no_words)\n",
        "fdist=dict(zip(*np.unique(most_occur, return_counts=True)))\n",
        "list_tags = list(fdist)[-no_words:]\n",
        "\n",
        "df_cv['Tags2'] = df_cv['Tags'].apply(lambda x: [tag for tag in list_tags if tag in x.split(\" \")])\n",
        "\n",
        "index_list = []\n",
        "\n",
        "for i, row in df_cv.iterrows():\n",
        "  if len(row['Tags2']) == 0:\n",
        "    index_list.append(i) \n",
        "\n",
        "df_cv.drop(index_list, axis=0, inplace=True)\n",
        "\n",
        "X = df_cv[['TitleBody']]\n",
        "y = df_cv[['Tags2']]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=list_tags)\n",
        "y_train = mlb.fit_transform(y_train['Tags2'])\n",
        "y_test = mlb.transform(y_test['Tags2'])\n",
        "\n",
        "mlb.classes_"
      ],
      "metadata": {
        "id": "suUpifui8tIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a66f4bc-5bc1-4c9c-b546-491cdefd2f4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['584', 'android', 'asp.net', 'c', 'c#', 'c++', 'cocoa-touch',\n",
              "       'html', 'ios', 'iphone', 'java', 'javascript', 'jquery', 'linux',\n",
              "       'objective-c', 'performance', 'php', 'python', 'sql', 'windows'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of word"
      ],
      "metadata": {
        "id": "L9VU3Er7AFh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train0 = X_train.copy()\n",
        "X_test0 = X_test.copy()\n",
        "\n",
        "count_vect = CountVectorizer(max_features=1000, binary=True)\n",
        "\n",
        "X_train_counts = count_vect.fit_transform(X_train0['TitleBody'])\n",
        "X_test_counts = count_vect.transform(X_test0['TitleBody']) # transform seulement"
      ],
      "metadata": {
        "id": "MXY2ZDHEAFTn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RÃ©duction dimensionnelle"
      ],
      "metadata": {
        "id": "sLJDqtP3HqOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qt = [d.split() for d in X_train0['TitleBody']]\n",
        "gensim_dictionary = corpora.Dictionary(qt)\n",
        "texts = qt\n",
        "gensim_corpus = [gensim_dictionary.doc2bow(text) for text in texts]\n",
        "print(gensim_corpus[:3])\n",
        "\n",
        "[[(gensim_dictionary[id], freq) for id, freq in cp] for cp in gensim_corpus[:4]] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvA33vulHqBh",
        "outputId": "6616b51a-80f5-4abc-b86e-ac8b6f4fcb40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 3), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 4), (11, 1), (12, 1), (13, 1), (14, 4), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 3), (22, 3), (23, 1), (24, 2), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 3), (35, 3), (36, 3), (37, 4), (38, 3), (39, 1), (40, 1), (41, 1), (42, 1), (43, 5), (44, 5), (45, 1), (46, 1), (47, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 5), (61, 4), (62, 14), (63, 1), (64, 1), (65, 1)], [(27, 2), (66, 1), (67, 9), (68, 1), (69, 2), (70, 1), (71, 1), (72, 1), (73, 1), (74, 2), (75, 4), (76, 2), (77, 3), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 3), (87, 1), (88, 1), (89, 1), (90, 1), (91, 2), (92, 1), (93, 1), (94, 6), (95, 1), (96, 1), (97, 2), (98, 1), (99, 2), (100, 3), (101, 1)], [(0, 1), (58, 1), (66, 1), (102, 2), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 5), (111, 1), (112, 2), (113, 2), (114, 1), (115, 2), (116, 2), (117, 3), (118, 1), (119, 3), (120, 1), (121, 2), (122, 1)]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('1', 3),\n",
              "  ('15', 1),\n",
              "  ('2', 1),\n",
              "  ('2d', 2),\n",
              "  ('according', 1),\n",
              "  ('achieve', 1),\n",
              "  ('anyone', 1),\n",
              "  ('border', 2),\n",
              "  ('code', 1),\n",
              "  ('colored', 1),\n",
              "  ('colorize', 4),\n",
              "  ('compute', 1),\n",
              "  ('created', 1),\n",
              "  ('data', 1),\n",
              "  ('diagram', 4),\n",
              "  ('docs', 1),\n",
              "  ('e', 1),\n",
              "  ('figure', 1),\n",
              "  ('fill', 2),\n",
              "  ('forming', 1),\n",
              "  ('help', 1),\n",
              "  ('image', 3),\n",
              "  ('import', 3),\n",
              "  ('indicates', 1),\n",
              "  ('indices', 2),\n",
              "  ('ints', 1),\n",
              "  ('list', 2),\n",
              "  ('make', 1),\n",
              "  ('matplotlib', 1),\n",
              "  ('need', 1),\n",
              "  ('np', 2),\n",
              "  ('nregions', 1),\n",
              "  ('numpy', 1),\n",
              "  ('order', 1),\n",
              "  ('outside', 3),\n",
              "  ('plot', 3),\n",
              "  ('plt', 3),\n",
              "  ('points', 4),\n",
              "  ('polygon', 3),\n",
              "  ('pyplot', 1),\n",
              "  ('rand', 1),\n",
              "  ('random', 1),\n",
              "  ('reasonably', 1),\n",
              "  ('region', 5),\n",
              "  ('regions', 5),\n",
              "  ('remove', 1),\n",
              "  ('resulting', 1),\n",
              "  ('scipy', 2),\n",
              "  ('see', 1),\n",
              "  ('seem', 1),\n",
              "  ('set', 1),\n",
              "  ('shape', 1),\n",
              "  ('show', 1),\n",
              "  ('spatial', 2),\n",
              "  ('tesselation', 1),\n",
              "  ('think', 1),\n",
              "  ('tried', 1),\n",
              "  ('trying', 1),\n",
              "  ('using', 1),\n",
              "  ('vertex', 1),\n",
              "  ('vertices', 5),\n",
              "  ('vor', 4),\n",
              "  ('voronoi', 14),\n",
              "  ('well', 1),\n",
              "  ('work', 1),\n",
              "  ('zip', 1)],\n",
              " [('make', 2),\n",
              "  ('?', 1),\n",
              "  ('abstract', 9),\n",
              "  ('allow', 1),\n",
              "  ('base', 2),\n",
              "  ('begin', 1),\n",
              "  ('called', 1),\n",
              "  ('calls', 1),\n",
              "  ('case', 1),\n",
              "  ('class', 2),\n",
              "  ('classes', 4),\n",
              "  ('constant', 2),\n",
              "  ('could', 3),\n",
              "  ('create', 1),\n",
              "  ('derived', 1),\n",
              "  ('designed', 1),\n",
              "  ('error', 1),\n",
              "  ('example', 1),\n",
              "  ('except', 1),\n",
              "  ('extend', 1),\n",
              "  ('field', 1),\n",
              "  ('fields', 3),\n",
              "  ('geterrmsg', 1),\n",
              "  ('happens', 1),\n",
              "  ('identical', 1),\n",
              "  ('instead', 1),\n",
              "  ('java', 2),\n",
              "  ('like', 1),\n",
              "  ('message', 1),\n",
              "  ('method', 6),\n",
              "  ('methods', 1),\n",
              "  ('override', 1),\n",
              "  ('pull', 2),\n",
              "  ('returns', 1),\n",
              "  ('string', 2),\n",
              "  ('two', 3),\n",
              "  ('within', 1)],\n",
              " [('1', 1),\n",
              "  ('using', 1),\n",
              "  ('?', 1),\n",
              "  (\"''\", 2),\n",
              "  ('/', 1),\n",
              "  ('2009', 1),\n",
              "  ('<', 1),\n",
              "  ('>', 1),\n",
              "  ('achieved', 1),\n",
              "  ('analytics', 1),\n",
              "  ('announced', 1),\n",
              "  ('async', 5),\n",
              "  ('async=', 1),\n",
              "  ('asynchronous', 2),\n",
              "  ('browsers', 2),\n",
              "  ('december', 1),\n",
              "  ('directive', 2),\n",
              "  ('google', 2),\n",
              "  ('script', 3),\n",
              "  ('since', 1),\n",
              "  ('support', 3),\n",
              "  ('tag', 1),\n",
              "  ('tracking', 2),\n",
              "  ('version', 1)],\n",
              " [('created', 2),\n",
              "  ('data', 2),\n",
              "  ('import', 1),\n",
              "  ('need', 2),\n",
              "  ('using', 2),\n",
              "  ('class', 1),\n",
              "  ('classes', 2),\n",
              "  ('create', 2),\n",
              "  ('based', 1),\n",
              "  ('boilerplate', 1),\n",
              "  ('build', 1),\n",
              "  ('coding', 1),\n",
              "  ('countryrepository', 1),\n",
              "  ('database', 1),\n",
              "  ('exactly', 1),\n",
              "  ('extends', 1),\n",
              "  ('facility', 9),\n",
              "  ('facilityrepository', 3),\n",
              "  ('facilityservice', 2),\n",
              "  ('facilityserviceimpl', 1),\n",
              "  ('fact', 1),\n",
              "  ('following', 1),\n",
              "  ('generics', 2),\n",
              "  ('go', 1),\n",
              "  ('good', 1),\n",
              "  ('idea', 1),\n",
              "  ('implements', 1),\n",
              "  ('interface', 2),\n",
              "  ('jpa', 3),\n",
              "  ('jparepository', 2),\n",
              "  ('long', 1),\n",
              "  ('lot', 1),\n",
              "  ('manage', 1),\n",
              "  ('may', 1),\n",
              "  ('multiple', 1),\n",
              "  ('number', 1),\n",
              "  ('object', 3),\n",
              "  ('occurred', 1),\n",
              "  ('org', 1),\n",
              "  ('persisted', 1),\n",
              "  ('persistence', 1),\n",
              "  ('possible', 1),\n",
              "  ('private', 1),\n",
              "  ('public', 5),\n",
              "  ('replace', 1),\n",
              "  ('repositories', 1),\n",
              "  ('repository', 1),\n",
              "  ('resource', 1),\n",
              "  ('return', 1),\n",
              "  ('save', 1),\n",
              "  ('saving', 1),\n",
              "  ('service', 1),\n",
              "  ('simple', 1),\n",
              "  ('spring', 2),\n",
              "  ('springframework', 1),\n",
              "  ('sure', 1),\n",
              "  ('three', 1),\n",
              "  ('thus', 1),\n",
              "  ('transactional', 1),\n",
              "  ('type', 2),\n",
              "  ('types', 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "\n",
        "while i < 10: #calculating and displaying the coherence score\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=i, random_state=100, \n",
        "    update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True\n",
        "  )\n",
        "\n",
        "  coherence_model_lda = CoherenceModel(\n",
        "    model=lda_model, texts=qt, dictionary=gensim_dictionary, coherence='c_v')\n",
        "  \n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  print('\\nCoherence Score :', coherence_lda, '// i =', i)\n",
        "  i = i + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4GoPY5EH50j",
        "outputId": "7110ed05-02b7-4cfa-c2c6-dfc8a3cc6a94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Coherence Score : 0.36568419717585815 // i = 1\n",
            "\n",
            "Coherence Score : 0.3937934338982566 // i = 2\n",
            "\n",
            "Coherence Score : 0.4713507152241547 // i = 3\n",
            "\n",
            "Coherence Score : 0.48230897208942974 // i = 4\n",
            "\n",
            "Coherence Score : 0.4919717130549627 // i = 5\n",
            "\n",
            "Coherence Score : 0.47621885201636943 // i = 6\n",
            "\n",
            "Coherence Score : 0.4395823651640512 // i = 7\n",
            "\n",
            "Coherence Score : 0.5053347873156604 // i = 8\n",
            "\n",
            "Coherence Score : 0.4772248376021915 // i = 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "        n_components=8, # 8 is the best feat according to the results above (highest coherence score is the 7th iteration)\n",
        "        max_iter=5, \n",
        "        learning_method='online', \n",
        "        learning_offset=50.,\n",
        "        random_state=0)\n",
        "\n",
        "lda.fit(X_train_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-W45OzDI-DU",
        "outputId": "a681853d-524a-435d-e3c0-9d79162d3f66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.15521493,  0.42677282,  0.16711351, ...,  0.17815212,\n",
              "         0.15227167,  0.16285152],\n",
              "       [ 5.13533656, 12.44873603, 12.91274601, ..., 18.12142426,\n",
              "        22.31397659,  6.30779503],\n",
              "       [17.27110235, 65.92815089, 17.62842508, ...,  2.10825935,\n",
              "        20.31648797, 49.34491423],\n",
              "       ...,\n",
              "       [ 0.59921003, 14.54610027, 40.45331463, ..., 18.02052474,\n",
              "        19.48397963,  2.55916438],\n",
              "       [ 1.25917293, 41.52452993, 12.30689525, ...,  0.15608339,\n",
              "         0.89125629,  7.81036768],\n",
              "       [ 0.15249747,  0.3164751 ,  0.3878486 , ...,  0.15413686,\n",
              "         0.15278786,  0.15168206]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic {}:\".format(topic_idx))\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        #print(topic)\n",
        "\n",
        "no_top_words = 10\n",
        "display_topics(lda, count_vect.get_feature_names_out(), no_top_words) # we can adjust the output by playing with max features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXWnwwBcJkOU",
        "outputId": "838701e9-59d6-4cba-d86d-f6b2f336ed76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:\n",
            "equivalent php use mac go line using allows specific hash\n",
            "Topic 1:\n",
            "new class get public code using return string function use\n",
            "Topic 2:\n",
            "using use like way would get file code need one\n",
            "Topic 3:\n",
            "table sql database select query column data visual using like\n",
            "Topic 4:\n",
            "image view ios height width like text screen size set\n",
            "Topic 5:\n",
            "code int use one using like way return function performance\n",
            "Topic 6:\n",
            "32 convert bit output need 64 got 16 display float\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSOLETE : On a du dÃ©veloppement (web en partie avec deux langages web) avec git, et plutÃ´t du vocabulaire avec tout ce qui concerne les bases de donnÃ©es."
      ],
      "metadata": {
        "id": "yRSjuAxoLj9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PrÃ©dictions"
      ],
      "metadata": {
        "id": "yNDp3d8NAFFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc=OneVsRestClassifier(RandomForestClassifier(random_state=42))\n",
        "\n",
        "rfc.fit(X_train_counts, y_train)\n",
        "\n",
        "pred=rfc.predict(X_test_counts)\n",
        "\n",
        "print('Jaccard score', jaccard_score(y_test,pred, average='micro'))\n",
        "print('Hamming loss', hamming_loss(y_test, pred))\n",
        "print('F1 score', f1_score(y_test,pred, average='micro'))\n",
        "\n",
        "#mlb.inverse_transform(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp-mmED4ADPG",
        "outputId": "bc4dfe2a-e31c-4dc8-b502-965fca496d16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py:80: UserWarning: Label not 0 is present in all training examples.\n",
            "  \"Label %s is present in all training examples.\" % str(classes[c])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard score 0.29797703663203934\n",
            "Hamming loss 0.05027407987470634\n",
            "F1 score 0.4591406908171863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "S4qA8ESw5x5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train0 = X_train.copy()\n",
        "X_test0 = X_test.copy()\n",
        "\n",
        "X_train0['TitleBody'] = X_train0['TitleBody'].apply(lambda x: x.split())\n",
        "wv = Word2Vec(X_train0['TitleBody'], min_count=2)\n",
        "\n",
        "def get_vect(word, model):\n",
        "    try:\n",
        "        return model.wv[word]\n",
        "    except KeyError:\n",
        "        return np.zeros((model.vector_size,))\n",
        "\n",
        "def sum_vectors(phrase, model):\n",
        "    return sum(get_vect(w, model) for w in phrase)\n",
        "\n",
        "def word2vec_features(X, model):\n",
        "    feats = np.vstack([sum_vectors(p, model) for p in X])\n",
        "    return feats\n",
        "\n",
        "wv_train_feat = word2vec_features(X_train0[\"TitleBody\"], wv)\n",
        "wv_test_feat = word2vec_features(X_test0[\"TitleBody\"], wv)"
      ],
      "metadata": {
        "id": "ZqdzMTlI5vgi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PrÃ©dictions"
      ],
      "metadata": {
        "id": "-fCDLsS16MsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc=OneVsRestClassifier(RandomForestClassifier(random_state=42))\n",
        "\n",
        "rfc.fit(wv_train_feat, y_train)\n",
        "\n",
        "pred=rfc.predict(wv_test_feat)\n",
        "\n",
        "print('Jaccard score', jaccard_score(y_test,pred, average='micro'))\n",
        "print('Hamming loss', hamming_loss(y_test, pred))\n",
        "print('F1 score', f1_score(y_test,pred, average='micro'))\n",
        "\n",
        "#mlb.inverse_transform(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xfAu4zr6Mac",
        "outputId": "87f3ef6a-fe4d-4be1-a457-c9a6425ef637"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py:80: UserWarning: Label not 0 is present in all training examples.\n",
            "  \"Label %s is present in all training examples.\" % str(classes[c])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard score 0.017671517671517672\n",
            "Hamming loss 0.0740015661707126\n",
            "F1 score 0.03472931562819203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USE"
      ],
      "metadata": {
        "id": "sVUyUBLb7_f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train0 = X_train.copy()\n",
        "X_test0 = X_test.copy()\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "#X_train0['TitleBody'] = X_train0['TitleBody'].apply(lambda x: [x])\n",
        "\n",
        "X_train_embed = X_train0['TitleBody'].to_list()\n",
        "X_train_embed = embed(X_train_embed)\n",
        "X_train_embed = np.array(X_train_embed)\n",
        "\n",
        "X_test_embed = X_test0['TitleBody'].to_list()\n",
        "X_test_embed = embed(X_test_embed)\n",
        "X_test_embed = np.array(X_test_embed)"
      ],
      "metadata": {
        "id": "okQ21nAB6aw5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PrÃ©dictions"
      ],
      "metadata": {
        "id": "rhf8RQXe8X9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc=OneVsRestClassifier(RandomForestClassifier(random_state=42))\n",
        "\n",
        "rfc.fit(X_train_embed, y_train)\n",
        "\n",
        "pred=rfc.predict(X_test_embed)\n",
        "\n",
        "print('Jaccard score', jaccard_score(y_test,pred, average='micro'))\n",
        "print('Hamming loss', hamming_loss(y_test, pred))\n",
        "print('F1 score', f1_score(y_test,pred, average='micro'))\n",
        "\n",
        "#mlb.inverse_transform(pred)"
      ],
      "metadata": {
        "id": "-9LhKxsX8ZEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c0de6e-29e9-4176-c494-ab67fc35a8a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py:80: UserWarning: Label not 0 is present in all training examples.\n",
            "  \"Label %s is present in all training examples.\" % str(classes[c])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard score 0.3656957928802589\n",
            "Hamming loss 0.04604541895066562\n",
            "F1 score 0.5355450236966824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "dYNyk-p88bkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train0 = X_train.copy()\n",
        "X_test0 = X_test.copy()\n",
        "\n",
        "# Using DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (TFDistilBertModel, DistilBertTokenizerFast, 'distilbert-base-uncased')\n",
        "\n",
        "pretrained_bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "def get_pretrained_bert_model(config=pretrained_weights):\n",
        "    if not config:\n",
        "        config = DistilBertConfig(num_labels=2)\n",
        "\n",
        "    return model_class.from_pretrained(pretrained_weights, config=config)\n",
        "\n",
        "def tokenize_encode(questions, max_length=None):\n",
        "    return pretrained_bert_tokenizer(\n",
        "        questions,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"tf\",\n",
        "    )\n",
        "\n",
        "# need to be explicit about the lengths (instead of just specifying padding=True in the tokenizer)\n",
        "# otherwise train questions end up being 71 and validation questions end up as 70, which causes problems/warnings\n",
        "max_length_question = 72\n",
        "max_length_keyword = 8\n",
        "\n",
        "train_questions_encoded = tokenize_encode(X_train0[\"TitleBody\"].to_list(), max_length_question) \n",
        "validation_questions_encoded = tokenize_encode(X_test0[\"TitleBody\"].to_list(), max_length_question) \n",
        "train_inputs_encoded = dict(train_questions_encoded)\n",
        "validation_inputs_encoded = dict(validation_questions_encoded)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (dict(train_questions_encoded), y_train))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (dict(validation_questions_encoded), y_test))\n",
        "\n",
        "train_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_inputs_encoded, y_train))\n",
        "\n",
        "val_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (validation_inputs_encoded, y_test))\n",
        "\n",
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
        "    min_df=1, ngram_range=(1, 1), norm=\"l2\")\n",
        "\n",
        "train_vectors = tfidf_vectorizer.fit_transform(raw_documents=X_train0[\"TitleBody\"]).toarray()\n",
        "validation_vectors = tfidf_vectorizer.transform(X_test0[\"TitleBody\"]).toarray()"
      ],
      "metadata": {
        "id": "veH0oUQc8fxL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PrÃ©dictions"
      ],
      "metadata": {
        "id": "U86i4B8190Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc=OneVsRestClassifier(RandomForestClassifier(random_state=42))\n",
        "\n",
        "rfc.fit(X_train_embed, y_train)\n",
        "\n",
        "pred=rfc.predict(X_test_embed)\n",
        "\n",
        "print('Jaccard score', jaccard_score(y_test,pred, average='micro'))\n",
        "print('Hamming loss', hamming_loss(y_test, pred))\n",
        "print('F1 score', f1_score(y_test,pred, average='micro'))\n",
        "\n",
        "#mlb.inverse_transform(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmV6Hgf691tw",
        "outputId": "f521619c-d8e8-4546-dd39-0ccf6febafe0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py:80: UserWarning: Label not 0 is present in all training examples.\n",
            "  \"Label %s is present in all training examples.\" % str(classes[c])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard score 0.3656957928802589\n",
            "Hamming loss 0.04604541895066562\n",
            "F1 score 0.5355450236966824\n"
          ]
        }
      ]
    }
  ]
}